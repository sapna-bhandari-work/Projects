```markdown
# Multithreaded Web Crawler in Python

## Description

This project is a Python-based multithreaded web crawler developed to crawl a target website and enumerate all internal links belonging to the same domain. The crawler uses a queue-based architecture and multiple worker threads to improve crawling efficiency while ensuring that each URL is visited only once.

The program is designed for educational purposes to demonstrate concepts such as multithreading, synchronization, web scraping, and graceful program termination.

---

## Key Features

- Multithreaded crawling using Python threading
- Queue-based task management for thread safety
- Crawls only internal links within the target domain
- Prevents duplicate URL visits using a set
- Graceful shutdown on keyboard interruption (Ctrl + C)
- Lightweight and console-based output

---

## Technologies Used

- Python 3
- urllib.request
- BeautifulSoup (bs4)
- threading
- queue
- signal

---

## Project Structure

```

.
├── webcrawler.py
└── README.md

````

---

## Requirements

- Python 3.x
- BeautifulSoup library

Install the required dependency using:

```bash
pip install beautifulsoup4
````

---

## Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/your-username/projects.git
   ```

2. Navigate to the project directory:

   ```bash
   cd projects
   ```

3. Ensure Python 3 is installed:

   ```bash
   python --version
   ```

---

## Usage

Run the crawler from the command line by providing the target website URL:

```bash
python webcrawler.py http://www.example.com/
```

---

## Usage Notes

* The target URL must start with `http://` or `https://`
* A trailing slash (`/`) is recommended for accurate link resolution
* Crawling continues until all internal links are processed or execution is stopped manually
* The program terminates cleanly when `Ctrl + C` is pressed
* No output files are generated by default

---

## Output

The crawler prints each discovered internal URL directly to the console.

Example output:

```
http://www.example.com/about
http://www.example.com/contact
http://www.example.com/services
```

---

## Working Explanation

1. The root URL is added to a thread-safe queue.
2. Multiple worker threads retrieve URLs from the queue.
3. Each URL is fetched using `urllib.request`.
4. HTML content is parsed using BeautifulSoup.
5. All anchor (`<a>`) tags are extracted.
6. Relative links are converted to absolute URLs.
7. External domain links are ignored.
8. Newly discovered internal links are added to the queue.
9. Previously visited links are skipped using a set.
10. Threads continue processing until the queue is empty or execution is interrupted.

---

## Limitations

* Does not respect robots.txt
* No crawl depth limitation
* No rate limiting
* No keyword or content analysis
* No persistent storage of crawled URLs

---

## Future Enhancements

* Implement crawl depth control
* Add keyword-based content searching
* Save crawled URLs to a file
* Introduce robots.txt compliance
* Add logging and improved error handling

---

## Disclaimer

This project is intended strictly for educational and learning purposes.
Use this tool only on websites where you have explicit permission to crawl.
The author is not responsible for any misuse of this software.

---

## Author

Sapna Bhandari
Third Year B.Sc. Computer Engineering Student

```
