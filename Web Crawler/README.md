# Web Crawler

## Description

This project is a Python-based multithreaded web crawler developed to crawl a target website and enumerate all internal links belonging to the same domain. The crawler uses a queue-based architecture and multiple worker threads to improve crawling efficiency while ensuring that each URL is visited only once.

The project is designed for educational purposes and demonstrates core concepts such as multithreading, web scraping, synchronization, and graceful program termination.

---

## Key Features

- Multithreaded crawling using Python threading  
- Queue-based task management for thread safety  
- Crawls only internal links within the target domain  
- Prevents duplicate URL visits using a set  
- Graceful shutdown on keyboard interruption (Ctrl + C)  
- Console-based output with no external storage  

---

## Technologies Used

- Python 3  
- urllib.request  
- BeautifulSoup (bs4)  
- threading  
- queue  
- signal  

---

## Project Structure

Project Folder
├── webcrawler.py
└── README

yaml
Copy code

---

## Requirements

- Python 3.x  
- BeautifulSoup library  

To install the required dependency:

pip install beautifulsoup4

yaml
Copy code

---

## Installation

Clone the repository:

git clone https://github.com/your-username/projects.git

css
Copy code

Navigate to the project directory:

cd projects

yaml
Copy code

Verify Python installation:

python --version

yaml
Copy code

---

## Usage

Run the crawler from the command line by providing the target website URL:

python webcrawler.py http://www.example.com/

yaml
Copy code

---

## Usage Notes

- The target URL must start with `http://` or `https://`  
- A trailing slash (`/`) is recommended for correct link resolution  
- Crawling continues until all internal links are processed or execution is stopped manually  
- The program terminates cleanly when Ctrl + C is pressed  
- No output files are generated by default  

---

## Output

The crawler prints each discovered internal URL directly to the console.

Example output:

http://www.example.com/about
http://www.example.com/contact
http://www.example.com/services

yaml
Copy code

---

## Working Explanation

1. The root URL is added to a thread-safe queue.  
2. Multiple worker threads retrieve URLs from the queue.  
3. Each URL is fetched using `urllib.request`.  
4. HTML content is parsed using BeautifulSoup.  
5. Anchor (`a`) tags are extracted from the page.  
6. Relative links are converted into absolute URLs.  
7. External domain links are ignored.  
8. Newly discovered internal links are added to the queue.  
9. Previously visited links are skipped using a set.  
10. Threads continue execution until the queue is empty or the program is interrupted.  

---

## Limitations

- Does not respect robots.txt  
- No crawl depth limitation  
- No rate limiting  
- No keyword or content analysis  
- No persistent storage of crawled URLs  

---

## Future Enhancements

- Implement crawl depth control  
- Add keyword-based content searching  
- Save crawled URLs to a file  
- Introduce robots.txt compliance  
- Improve logging and error handling  

---

## Disclaimer

This project is intended strictly for educational and learning purposes.  
Use this tool only on websites where you have explicit permission to crawl.  
The author is not responsible for any misuse of this software.

---

## Author

Sapna Bhandari  
Third Year B.Sc. Computer Engineering Student
